{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c44dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import mdptoolbox.example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a949ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    def __init__(self, Q=5, Tp=3, Tnp=3, p=0.5, P1=0.3, P2=0.5, W1=5, W2=3, arrival_rate=0.2):\n",
    "        self.Q = Q\n",
    "        self.Tp = Tp ## priority time\n",
    "        self.Tnp = Tnp ## non-priority time\n",
    "        self.W1 = W1 ## cost of priority\n",
    "        self.W2 = W2 ## cost of non-priority\n",
    "        self.arrival_rate = arrival_rate \n",
    "        self.p = p\n",
    "        self.P1 = (1-np.exp(-Tp*arrival_rate))\n",
    "        self.P2 = (1-np.exp(-Tnp*arrival_rate))\n",
    "        if arrival_rate * Tp > 1 or arrival_rate * Tnp > 1:\n",
    "            print('Arrival rate too high')\n",
    "            raise AssertionError(\"Arrival rate too high\")\n",
    "        \n",
    "        self.action_times = {\n",
    "            \"0\" : 1 / arrival_rate,\n",
    "            \"1\" : Tp,\n",
    "            \"2\" : Tnp\n",
    "        }\n",
    "        self.states = [(i, j) for i in range(self.Q + 1) for j in range(self.Q - i + 1)]\n",
    "            \n",
    "    def sample_next_state(self, state, action):\n",
    "        priority_state = state[0]\n",
    "        non_priority_state = state[1]\n",
    "        ## Hold state\n",
    "        if action == 0:\n",
    "            if (priority_state + non_priority_state) == self.Q:\n",
    "                cost = 1e12\n",
    "                prob = 1\n",
    "                arrival_time = (1 / self.arrival_rate)\n",
    "            else:\n",
    "                arrival_time = 1 / self.arrival_rate\n",
    "                cost = (self.W1*priority_state + self.W2*non_priority_state) * (1 / self.arrival_rate)\n",
    "                if random.random() < self.p:\n",
    "                    # priority arrival\n",
    "                    priority_state += 1\n",
    "                    prob = self.p\n",
    "                else:\n",
    "                    non_priority_state += 1\n",
    "                    prob = 1- self.p\n",
    "            arrival_time = (1 / self.arrival_rate)\n",
    "                \n",
    "        if action == 1:\n",
    "            if (priority_state == 0):\n",
    "                cost = 1e12\n",
    "                prob = 1\n",
    "                arrival_time = self.Tp\n",
    "            else:\n",
    "                arrival_time = np.random.exponential(1 / self.arrival_rate)\n",
    "                arrival_time = arrival_time if arrival_time < self.Tp else self.Tp\n",
    "                priority_state -= 1\n",
    "                arrival_time = self.Tp\n",
    "                if random.random() < self.P1:\n",
    "                    # Probability of Arrivals\n",
    "                    prob = self.P1\n",
    "                    if random.random() < self.p:\n",
    "                        # priority arrival\n",
    "                        priority_state += 1\n",
    "                        cost =(self.W1*priority_state + self.W2*non_priority_state) * self.Tp+ (self.W1 * (self.Tp- ((1-np.exp(-self.arrival_rate*self.Tp))/(self.arrival_rate))))\n",
    "                        prob *= self.p\n",
    "                    else:\n",
    "                        # non priority arrival\n",
    "                        non_priority_state += 1\n",
    "                        cost = (self.W1*priority_state + self.W2*non_priority_state) * self.Tp+(self.W2 * (self.Tp-((1-np.exp(-self.arrival_rate*self.Tp))/(self.arrival_rate))))\n",
    "                        prob *= (1-self.p)\n",
    "                else:\n",
    "                    # No arrival\n",
    "                    prob = 1 - self.P1\n",
    "                    cost =(self.W1*priority_state + self.W2*non_priority_state) * self.Tp\n",
    "            arrival_time=self.Tp\n",
    "                    \n",
    "        if action == 2:\n",
    "            if (non_priority_state == 0):\n",
    "                cost = 1e12\n",
    "                prob = 1\n",
    "                arrival_time = 0\n",
    "            else:\n",
    "                arrival_time = np.random.exponential(1 / self.arrival_rate)\n",
    "                arrival_time = arrival_time if arrival_time < self.Tnp else self.Tnp\n",
    "                non_priority_state -= 1\n",
    "                arrival_time = self.Tnp\n",
    "                if random.random() < self.P2:\n",
    "                    # Probability of Arrivals\n",
    "                    prob = self.P2\n",
    "                    if random.random() < self.p:\n",
    "                        # priority arrival\n",
    "                        priority_state += 1\n",
    "                        cost =(self.W1*priority_state + self.W2*non_priority_state) * self.Tnp + (self.W1 * (self.Tnp-((1-np.exp(-self.arrival_rate*self.Tnp))/(self.arrival_rate))))\n",
    "                        prob *= self.p\n",
    "                    else:\n",
    "                        # non priority arrival\n",
    "                        non_priority_state += 1\n",
    "                        cost = (self.W1*priority_state + self.W2*non_priority_state) * self.Tnp+(self.W2 * (self.Tnp-((1-np.exp(-self.arrival_rate*self.Tnp))/(self.arrival_rate))))\n",
    "                        prob *= (1-self.p)\n",
    "                else:\n",
    "                    # No arrival\n",
    "                    prob = 1 - self.P2\n",
    "                    cost = (self.W1*priority_state + self.W2*non_priority_state) * self.Tnp\n",
    "            arrival_time=self.Tnp\n",
    "                    \n",
    "#         return non_priority_state, cost, priority_state, prob, arrival_time \n",
    "        return priority_state, non_priority_state, cost, prob,arrival_time\n",
    "                \n",
    "    \n",
    "    def get_transition_set(self, state, action):\n",
    "        tset = set()\n",
    "        values ={}\n",
    "        count = {}\n",
    "        for _ in range(1000):\n",
    "#             non_priority, cost, params, prob, time_for_discounting = self.sample_next_state(state, action)\n",
    "            priority_state, non_priority_state, cost, prob,arrival_time = self.sample_next_state(state, action)\n",
    "            next_state = (priority_state, non_priority_state)\n",
    "#             if values.get(next_state) != None:\n",
    "#                 count[next_state] = count[next_state] + 1\n",
    "#                 values[next_state] = values[next_state] + (cost - values[next_state])/count[next_state]\n",
    "#             else:\n",
    "#                 count[next_state] = 1\n",
    "#                 values[next_state] = cost\n",
    "            tset.add((next_state, prob,cost,arrival_time))\n",
    "#         print(tset)\n",
    "#         output = self.sample_next_state(state,action)\n",
    "#         print(output)\n",
    "#         print(\"state:\",state,\"action:\",action)\n",
    "#         print(\"next_state_with_cost:\",values)\n",
    "        return tset\n",
    "    \n",
    "    def action_0(self, state):\n",
    "        if state[0] + state[1] == self.Q:\n",
    "            return [(state, 1, 1e12)]\n",
    "        cost = (self.W1 * state[0] + self.W2 * state[1])  / self.arrival_rate\n",
    "        transitions = [(((state[0] + 1, state[1])), self.p, cost), ((state[0], state[1] + 1), 1- self.p, cost)]\n",
    "        \n",
    "        return transitions\n",
    "    \n",
    "    def action_1(self, state):\n",
    "        if state[0] == 0:\n",
    "            return [(state , 1, 1e12)]\n",
    "        \n",
    "        transitions = []\n",
    "        # N.A\n",
    "        cost = ((state[0] - 1)* self.W1 + state[1] * self.W2) * self.Tp\n",
    "        transitions.append(((state[0] - 1, state[1]), 1 - self.P1, cost))\n",
    "\n",
    "        # P.A\n",
    "        priority_arrival_cost = self.W1 * (self.Tp- ((1-np.exp(-self.arrival_rate*self.Tp))/(self.arrival_rate)))\n",
    "        transitions.append((state, self.p * self.P1, cost + priority_arrival_cost))\n",
    "\n",
    "        # N.P.A\n",
    "        non_priority_cost = self.W2 * (self.Tp-((1-np.exp(-self.arrival_rate*self.Tp))/(self.arrival_rate)))\n",
    "        transitions.append(((state[0]-1, state[1] + 1), (1 - self.p) * self.P1, cost + non_priority_cost))\n",
    "        return transitions\n",
    "    \n",
    "    def action_2(self, state):\n",
    "        if state[1] == 0:\n",
    "            return [(state, 1, 1e12)]\n",
    "        transitions = []\n",
    "\n",
    "        # N.A\n",
    "        cost = (state[0] * self.W1 + (state[1] - 1) * self.W2) * self.Tnp\n",
    "        transitions.append(((state[0], state[1] - 1), 1 - self.P2, cost))\n",
    "\n",
    "        # P.A\n",
    "        priority_arrival_cost = (self.W1 * (self.Tnp-((1-np.exp(-self.arrival_rate*self.Tnp))/(self.arrival_rate))))\n",
    "        transitions.append(((state[0] + 1, state[1]-1), self.p * self.P2, cost + priority_arrival_cost))\n",
    "\n",
    "        # N.P.A\n",
    "        non_priority_cost = (self.W2 * (self.Tnp-((1-np.exp(-self.arrival_rate*self.Tnp))/(self.arrival_rate))))\n",
    "        transitions.append(((state[0], state[1]), (1 - self.p) * self.P2, cost + non_priority_cost))\n",
    "\n",
    "        return transitions\n",
    "    \n",
    "    def get_transition_reward_matrix(self):        \n",
    "\n",
    "        transition_reward_matrix = {}\n",
    "\n",
    "        for state in self.states:\n",
    "            action_0_result = self.action_0(state)\n",
    "            action_1_result = self.action_1(state)\n",
    "            action_2_result = self.action_2(state)\n",
    "            transition_reward_matrix[state] = (action_0_result, action_1_result, action_2_result)\n",
    "            transition_reward_matrix[state] = {\n",
    "                0 : action_0_result,\n",
    "                1 : action_1_result,\n",
    "                2 : action_2_result\n",
    "            }\n",
    "        return transition_reward_matrix\n",
    "\n",
    "    \n",
    "    def value_iteration(self, gamma=0.01, theta=1e-3, max_iterations=10000):\n",
    "        states = [[i, j] for i in range(self.Q + 1) for j in range(self.Q - i + 1)]\n",
    "        transition_reward_matrix = {}\n",
    "        actions = [0, 1, 2]\n",
    "\n",
    "        for state in states:\n",
    "            transition_reward_matrix[tuple(state)] = {}\n",
    "#             print(state)\n",
    "            for action in actions:\n",
    "                tset = self.get_transition_set(state, action)\n",
    "                transition_reward_matrix[tuple(state)][action] = tset\n",
    "        \n",
    "#         print(transition_reward_matrix)\n",
    "#         return\n",
    "        value_function = {tuple(state): 0 for state in transition_reward_matrix}\n",
    "        Q_values = {tuple(state): {action: 0 for action in actions} for state in transition_reward_matrix}\n",
    "        policy = {}\n",
    "\n",
    "        delta = float('inf')\n",
    "        iteration = 0\n",
    "        while delta > theta and iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"Iteration: {iteration}\")\n",
    "            delta = 0\n",
    "            for state, action_dict in transition_reward_matrix.items():\n",
    "                old_value = value_function[state]\n",
    "                action_values = []\n",
    "                for action, tset in action_dict.items():\n",
    "                    action_value = 0 \n",
    "                    for transition in tset:\n",
    "                        next_state = transition[0]\n",
    "                        prob = transition[1]\n",
    "                        cost = transition[2]\n",
    "                        time_for_discounting = transition[3]\n",
    "                        print(\"Here you go:state,action, next_state,prob,cost,time_for_discounting\",state,action, next_state,prob,cost,time_for_discounting)\n",
    "                        # print(state, next_state, prob)\n",
    "                        # time.sleep(10)\n",
    "#                         dummy_1, dummy_2, cost, dummy_3,time_for_discounting=self.sample_next_state(state, action)\n",
    "#                         next_state = (dummy_1,dummy_2)\n",
    "#                         prob = dummy_3\n",
    "#                         action_value += ( 1/(1+cost) + np.exp(-time_for_discounting*gamma) * value_function[next_state])*prob\n",
    "#                         action_value += ( 1/(1+cost) + 0.011* value_function[next_state])*prob\n",
    "#                         action_value += ( 1/(1+cost) + 0.11* value_function[next_state])*prob\n",
    "#                        action_value += (-1 * cost + 0.9 * value_function[next_state])*prob\n",
    "                        action_value += (-cost + np.exp(-time_for_discounting*gamma) * value_function[next_state])*prob\n",
    "                        \n",
    "                    Q_values[state][action] = action_value\n",
    "                    action_values.append(action_value)\n",
    "\n",
    "                value_function[state] = np.max(action_values)\n",
    "                policy[state] = np.argmax(action_values)\n",
    "                delta = max(delta, abs(value_function[state] - old_value))\n",
    "                print(f\"State: {state}, Value: {value_function[state]}, Policy: {policy[state]}\",)\n",
    "            \n",
    "#             for item in value_function:\n",
    "#                 value_function[item] -= value_function[(0,0)]\n",
    "\n",
    "            \n",
    "        return value_function, Q_values, policy, transition_reward_matrix\n",
    "    \n",
    "    \n",
    "    def export_transition_reward_matrix(self, transition_reward_matrix):\n",
    "        transition_list = []\n",
    "        reward_list = []\n",
    "        for state, action_dict in transition_reward_matrix.items():\n",
    "            for action, tset in action_dict.items():\n",
    "                for transition in tset:\n",
    "                    next_state, prob = transition\n",
    "                    transition_list.append((state, action, next_state, prob))\n",
    "                    # reward_list.append((state, action, next_state, cost))\n",
    "\n",
    "        transition_df = pd.DataFrame(transition_list, columns=[\"state\", \"action\", \"next_state\", \"probability\"])\n",
    "        # reward_df = pd.DataFrame(reward_list, columns=[\"state\", \"action\", \"next_state\", \"reward\"])\n",
    "\n",
    "        transition_df.to_csv(\"transition_matrix.csv\", index=False)\n",
    "        # reward_df.to_csv(\"reward_matrix.csv\", index=False)\n",
    "\n",
    "    def run_simulation_for_params(self,params):\n",
    "        \"\"\"\n",
    "        Run simulation for a given set of parameters and return the results.\n",
    "        \"\"\"\n",
    "        vi_instance = ValueIteration(**params)\n",
    "        _, Q_values, _, transition_reward_matrix = vi_instance.value_iteration()\n",
    "\n",
    "        results = []\n",
    "        for state, actions in Q_values.items():\n",
    "            for action, Q_value in actions.items():\n",
    "                result_row = {**params, 'state': state, 'action': action, 'Q_value': Q_value}\n",
    "                results.append(result_row)\n",
    "\n",
    "        # Export transition and reward matrices for this set of parameters\n",
    "        self.export_transition_reward_matrix(transition_reward_matrix)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def run_simulations(self, parameter_values):\n",
    "        param_names = list(parameter_values.keys())\n",
    "        param_combinations = list(product(*parameter_values.values()))\n",
    "\n",
    "        # Prepare the pool for multiprocessing\n",
    "        pool = mp.Pool(20)\n",
    " \n",
    "        # Create a list of dictionaries of parameters\n",
    "        param_dicts = [dict(zip(param_names, combination)) for combination in param_combinations]\n",
    "\n",
    "        # Step 1: Run simulations in parallel\n",
    "        all_results = pool.map(self.run_simulation_for_params, param_dicts)\n",
    "\n",
    "        # Step 2: Flatten the list of results\n",
    "        flat_results = [item for sublist in all_results for item in sublist]\n",
    "\n",
    "        # Step 3: Create DataFrame from results list\n",
    "        result_df = pd.DataFrame(flat_results)\n",
    "\n",
    "        # Save to CSV with all parameters as separate columns\n",
    "        result_df.to_csv(\"Q_values_simulations.csv\", index=False)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d7573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# asd = ValueIteration(5,3,3,0.5,0.3,0.5,405,3,0.2)\n",
    "\n",
    "# value_function, Q_values, _, transition_reward_matrix = asd.value_iteration()\n",
    "# # transition_reward_matrix.to_csv('transition_reward_matrix.csv')\n",
    "\n",
    "# import pandas as pd\n",
    "# dataframe = pd.DataFrame(transition_reward_matrix)\n",
    "# dataframe.to_csv(\"data.csv\", header=True)\n",
    "\n",
    "# # results = []\n",
    "# # for state, actions in Q_values.items():\n",
    "# #     for action, Q_value in actions.items():\n",
    "# #         result_row = {**params, 'state': state, 'action': action, 'Q_value': Q_value}\n",
    "# #         results.append(result_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70d0c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 5\n",
    "asd = ValueIteration(Q,3,3,0.5,0.3,0.5,35, 1,0.2)\n",
    "states = np.array([[i, j] for i in range(Q + 1) for j in range(Q - i + 1)])\n",
    "P1 = np.zeros((3,len(states),len(states)))\n",
    "R1 = np.zeros((3,len(states),len(states)))\n",
    "for actions in [0,1,2]:\n",
    "    for i in range(len(states)):\n",
    "        transition_set = asd.get_transition_set(states[i],actions)\n",
    "        for next_state,prob,cost,arrival_time in transition_set:\n",
    "            P1[actions,i,(states[:,0] == next_state[0]) & (states[:,1] == next_state[1])] = prob\n",
    "            R1[actions,i,(states[:,0] == next_state[0]) & (states[:,1] == next_state[1])] = -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b257dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 5\n",
    "asd = ValueIteration(Q,3,3,0.5,0.3,0.5,35, 1,0.2)\n",
    "transition_reward_matrix = asd.get_transition_reward_matrix()\n",
    "\n",
    "states = np.array([[i, j] for i in range(Q + 1) for j in range(Q - i + 1)])\n",
    "P2 = np.zeros((3,len(states),len(states)))\n",
    "R2 = np.zeros((3,len(states),len(states)))\n",
    "for actions in [0,1,2]:\n",
    "    for i in range(len(states)):\n",
    "        transition_set = transition_reward_matrix[tuple(states[i])][actions]\n",
    "        for next_state,prob,cost in transition_set:\n",
    "            P2[actions,i,(states[:,0] == next_state[0]) & (states[:,1] == next_state[1])] = prob\n",
    "            R2[actions,i,(states[:,0] == next_state[0]) & (states[:,1] == next_state[1])] = -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2b656d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:[0 0], Values:-82.17621120249632, Policy:0\n",
      "States:[0 1], Values:-102.59525507023973, Policy:0\n",
      "States:[0 2], Values:-122.22189963024346, Policy:0\n",
      "States:[0 3], Values:-140.57218005865835, Policy:2\n",
      "States:[0 4], Values:-157.87394052061387, Policy:2\n",
      "States:[0 5], Values:-174.45532861025714, Policy:2\n",
      "States:[1 0], Values:-103.43412619537075, Policy:1\n",
      "States:[1 1], Values:-122.35577109531951, Policy:1\n",
      "States:[1 2], Values:-140.57218005865835, Policy:1\n",
      "States:[1 3], Values:-157.87394052061387, Policy:1\n",
      "States:[1 4], Values:-174.45532861025714, Policy:1\n",
      "States:[2 0], Values:-247.2975907901182, Policy:1\n",
      "States:[2 1], Values:-265.10635876848886, Policy:1\n",
      "States:[2 2], Values:-282.3363339918648, Policy:1\n",
      "States:[2 3], Values:-298.91772208150223, Policy:1\n",
      "States:[3 0], Values:-483.9301747092224, Policy:1\n",
      "States:[3 1], Values:-500.9253747523716, Policy:1\n",
      "States:[3 2], Values:-517.4682200269737, Policy:1\n",
      "States:[4 0], Values:-790.7580897126643, Policy:1\n",
      "States:[4 1], Values:-807.1659525248031, Policy:1\n",
      "States:[5 0], Values:-1150.7004559860338, Policy:1\n"
     ]
    }
   ],
   "source": [
    "# P, R = mdptoolbox.example.forest()\n",
    "vi = mdptoolbox.mdp.ValueIteration(P1, R1,0.8)\n",
    "vi.run()\n",
    "result = vi.policy # result is (0, 0, 0)import mdptoolbox.example\n",
    "for i in range(len(states)):\n",
    "    print(f\"States:{states[i]}, Values:{vi.V[i]}, Policy:{result[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c6939ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:[0 0], Values:-16.235671756961818, Policy:0\n",
      "States:[0 1], Values:-20.43481821877366, Policy:2\n",
      "States:[0 2], Values:-28.014258859647782, Policy:2\n",
      "States:[0 3], Values:-37.91605281465993, Policy:2\n",
      "States:[0 4], Values:-49.413252034233054, Policy:2\n",
      "States:[0 5], Values:-62.00581838373112, Policy:2\n",
      "States:[1 0], Values:-20.43481821877366, Policy:1\n",
      "States:[1 1], Values:-28.014258859647782, Policy:1\n",
      "States:[1 2], Values:-37.91605281465993, Policy:1\n",
      "States:[1 3], Values:-49.413252034233054, Policy:1\n",
      "States:[1 4], Values:-62.00581838373112, Policy:1\n",
      "States:[2 0], Values:-152.47665233075298, Policy:1\n",
      "States:[2 1], Values:-162.3784462857473, Policy:1\n",
      "States:[2 2], Values:-173.87564550532042, Policy:1\n",
      "States:[2 3], Values:-186.4682118548185, Policy:1\n",
      "States:[3 0], Values:-380.9289442226703, Policy:1\n",
      "States:[3 1], Values:-392.426143441637, Policy:1\n",
      "States:[3 2], Values:-405.01870979113505, Policy:1\n",
      "States:[4 0], Values:-682.1031168476466, Policy:1\n",
      "States:[4 1], Values:-694.6956831875552, Policy:1\n",
      "States:[5 0], Values:-1038.1411454779122, Policy:1\n"
     ]
    }
   ],
   "source": [
    "# P, R = mdptoolbox.example.forest()\n",
    "vi = mdptoolbox.mdp.ValueIteration(P2, R2,0.8)\n",
    "vi.run()\n",
    "result = vi.policy # result is (0, 0, 0)import mdptoolbox.example\n",
    "for i in range(len(states)):\n",
    "    print(f\"States:{states[i]}, Values:{vi.V[i]}, Policy:{result[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50cd61da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tU variation\n",
      "    1\t\t  450.4069587266717\n",
      "    2\t\t  361.7835264882077\n",
      "    3\t\t  274.28976040215565\n",
      "    4\t\t  192.33581337876996\n",
      "    5\t\t  110.31650960454039\n",
      "    6\t\t  49.621511725821364\n",
      "    7\t\t  20.107764814885172\n",
      "    8\t\t  8.15916423431355\n",
      "    9\t\t  6.413079487635059\n",
      "    10\t\t  4.8238807846678355\n",
      "    11\t\t  3.454153415699011\n",
      "    12\t\t  2.34130647191364\n",
      "    13\t\t  1.5271782625022752\n",
      "    14\t\t  0.9578017076532888\n",
      "    15\t\t  0.5845741193425056\n",
      "    16\t\t  0.34683723544230816\n",
      "    17\t\t  0.2018324189480154\n",
      "    18\t\t  0.1149935439834735\n",
      "    19\t\t  0.06460020816712131\n",
      "    20\t\t  0.0356977844191988\n",
      "    21\t\t  0.01952200044820529\n",
      "    22\t\t  0.010534538472029453\n",
      "    23\t\t  0.005640823033445486\n",
      "Iterating stopped, epsilon-optimal policy found.\n",
      "States:[0 0], Values:1450.3203405495542, Policy:0\n",
      "States:[0 1], Values:1430.687889540296, Policy:2\n",
      "States:[0 2], Values:1405.5890946374022, Policy:2\n",
      "States:[0 3], Values:1375.0239994171407, Policy:2\n",
      "States:[0 4], Values:1338.9932258531674, Policy:2\n",
      "States:[0 5], Values:1297.5007915282304, Policy:2\n",
      "States:[1 0], Values:1430.687889540296, Policy:1\n",
      "States:[1 1], Values:1405.5890946374022, Policy:1\n",
      "States:[1 2], Values:1375.0239994171407, Policy:1\n",
      "States:[1 3], Values:1338.9932258531674, Policy:1\n",
      "States:[1 4], Values:1297.5007915282304, Policy:1\n",
      "States:[2 0], Values:1273.8752081673313, Policy:1\n",
      "States:[2 1], Values:1243.3101129470751, Policy:1\n",
      "States:[2 2], Values:1207.2793393831037, Policy:1\n",
      "States:[2 3], Values:1165.7869050581678, Policy:1\n",
      "States:[3 0], Values:979.8823400064848, Policy:1\n",
      "States:[3 1], Values:943.8515664427869, Policy:1\n",
      "States:[3 2], Values:902.3591321179752, Policy:1\n",
      "States:[4 0], Values:548.7099070207707, Policy:1\n",
      "States:[4 1], Values:507.2174727028896, Policy:1\n",
      "States:[5 0], Values:-19.638073367137423, Policy:1\n"
     ]
    }
   ],
   "source": [
    "rvi = mdptoolbox.mdp.RelativeValueIteration(P1,R1)\n",
    "rvi.setVerbose()\n",
    "\n",
    "rvi.run()\n",
    "result = rvi.policy # result is (0, 0, 0)import mdptoolbox.example\n",
    "for i in range(len(states)):\n",
    "    print(f\"States:{states[i]}, Values:{rvi.V[i]}, Policy:{result[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5eb62568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tU variation\n",
      "    1\t\t  426.0427870757492\n",
      "    2\t\t  342.9157702098371\n",
      "    3\t\t  261.7585483228851\n",
      "    4\t\t  180.5014945365885\n",
      "    5\t\t  99.86566618751112\n",
      "    6\t\t  47.19269806777902\n",
      "    7\t\t  20.63584803744095\n",
      "    8\t\t  8.80516187080758\n",
      "    9\t\t  3.894201119493175\n",
      "    10\t\t  1.8205058095664413\n",
      "    11\t\t  0.9181794524377551\n",
      "    12\t\t  0.4837262033216234\n",
      "    13\t\t  0.26499358561062536\n",
      "    14\t\t  0.14555570207511437\n",
      "    15\t\t  0.08072191342608903\n",
      "    16\t\t  0.04411927889839262\n",
      "    17\t\t  0.02413792794766323\n",
      "    18\t\t  0.012969398347649985\n",
      "    19\t\t  0.006971712867596125\n",
      "Iterating stopped, epsilon-optimal policy found.\n",
      "States:[0 0], Values:1387.394119779321, Policy:0\n",
      "States:[0 1], Values:1383.492523152957, Policy:2\n",
      "States:[0 2], Values:1374.1246246612286, Policy:2\n",
      "States:[0 3], Values:1359.290418384827, Policy:2\n",
      "States:[0 4], Values:1338.99070581555, Policy:2\n",
      "States:[0 5], Values:1313.2303865653698, Policy:2\n",
      "States:[1 0], Values:1383.492523152957, Policy:1\n",
      "States:[1 1], Values:1374.1246246612286, Policy:1\n",
      "States:[1 2], Values:1359.290418384827, Policy:1\n",
      "States:[1 3], Values:1338.99070581555, Policy:1\n",
      "States:[1 4], Values:1313.2303865653698, Policy:1\n",
      "States:[2 0], Values:1242.4107381912286, Policy:1\n",
      "States:[2 1], Values:1227.576531914833, Policy:1\n",
      "States:[2 2], Values:1207.2768193455556, Policy:1\n",
      "States:[2 3], Values:1181.5165000953755, Policy:1\n",
      "States:[3 0], Values:964.1487589790427, Policy:1\n",
      "States:[3 1], Values:943.8490464100057, Policy:1\n",
      "States:[3 2], Values:918.0887271598256, Policy:1\n",
      "States:[4 0], Values:548.7073871413561, Policy:1\n",
      "States:[4 1], Values:522.9470678958523, Policy:1\n",
      "States:[5 0], Values:-3.9084750859090036, Policy:1\n"
     ]
    }
   ],
   "source": [
    "rvi = mdptoolbox.mdp.RelativeValueIteration(P2,R2)\n",
    "rvi.setVerbose()\n",
    "\n",
    "rvi.run()\n",
    "result = rvi.policy # result is (0, 0, 0)import mdptoolbox.example\n",
    "for i in range(len(states)):\n",
    "    print(f\"States:{states[i]}, Values:{rvi.V[i]}, Policy:{result[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "445cb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as _np\n",
    "global check, time_for_discounting\n",
    "\n",
    "check = True\n",
    "class ValueIterationUpdated(mdptoolbox.mdp.ValueIteration):\n",
    "\n",
    "    def _bellmanOperator(self, V=None):\n",
    "        global time_for_discounting, check\n",
    "        if check:\n",
    "            print(\"Custom implementation is being used\")\n",
    "            check = False\n",
    "        # Apply the Bellman operator on the value function.\n",
    "        #\n",
    "        # Updates the value function and the Vprev-improving policy.\n",
    "        #\n",
    "        # Returns: (policy, value), tuple of new policy and its value\n",
    "        #\n",
    "        # If V hasn't been sent into the method, then we assume to be working\n",
    "        # on the objects V attribute\n",
    "        if V is None:\n",
    "            # this V should be a reference to the data rather than a copy\n",
    "            V = self.V\n",
    "        else:\n",
    "            # make sure the user supplied V is of the right shape\n",
    "            try:\n",
    "                assert V.shape in ((self.S,), (1, self.S)), \"V is not the \" \\\n",
    "                    \"right shape (Bellman operator).\"\n",
    "            except AttributeError:\n",
    "                raise TypeError(\"V must be a numpy array or matrix.\")\n",
    "        # Looping through each action the the Q-value matrix is calculated.\n",
    "        # P and V can be any object that supports indexing, so it is important\n",
    "        # that you know they define a valid MDP before calling the\n",
    "        # _bellmanOperator method. Otherwise the results will be meaningless.\n",
    "        Q = _np.empty((self.A, self.S))\n",
    "        for aa in range(self.A):\n",
    "            Q[aa] = self.R[aa] + np.exp(-time_for_discounting[aa]*self.discount) * self.P[aa].dot(V)\n",
    "        # Get the policy and value, for now it is being returned but...\n",
    "        # Which way is better?\n",
    "        # 1. Return, (policy, value)\n",
    "        return (Q.argmax(axis=0), Q.max(axis=0))\n",
    "        # 2. update self.policy and self.V directly\n",
    "        # self.V = Q.max(axis=1)\n",
    "        # self.policy = Q.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c14a6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom implementation is being used\n",
      "States:[0 0], Values:-0.3350415357126446, Policy:0\n",
      "States:[0 1], Values:-5.412926647988585, Policy:0\n",
      "States:[0 2], Values:-10.490827463780933, Policy:0\n",
      "States:[0 3], Values:-15.570408782979662, Policy:0\n",
      "States:[0 4], Values:-20.829663058725583, Policy:0\n",
      "States:[0 5], Values:-45.298096152401214, Policy:2\n",
      "States:[1 0], Values:-31.172373689028277, Policy:1\n",
      "States:[1 1], Values:-34.59924048823092, Policy:1\n",
      "States:[1 2], Values:-38.02614315319525, Policy:1\n",
      "States:[1 3], Values:-41.45688493823617, Policy:1\n",
      "States:[1 4], Values:-45.298096152401214, Policy:1\n",
      "States:[2 0], Values:-140.5433091437508, Policy:1\n",
      "States:[2 1], Values:-143.85176473023557, Policy:1\n",
      "States:[2 2], Values:-147.16030222683156, Policy:1\n",
      "States:[2 3], Values:-150.47761018064432, Policy:1\n",
      "States:[3 0], Values:-255.57871799311454, Policy:1\n",
      "States:[3 1], Values:-258.8786817442317, Policy:1\n",
      "States:[3 2], Values:-262.1788325491774, Policy:1\n",
      "States:[4 0], Values:-371.02267793385573, Policy:1\n",
      "States:[4 1], Values:-374.3220341768776, Policy:1\n",
      "States:[5 0], Values:-486.49609492129355, Policy:1\n"
     ]
    }
   ],
   "source": [
    "Q = 5\n",
    "asd = ValueIteration(Q,3,3,0.5,0.3,0.5,35, 1,0.2)\n",
    "states = np.array([[i, j] for i in range(Q + 1) for j in range(Q - i + 1)])\n",
    "P = np.zeros((3,len(states),len(states)))\n",
    "R = np.zeros((3,len(states),len(states)))\n",
    "for actions in [0,1,2]:\n",
    "    for i in range(len(states)):\n",
    "        transition_set = asd.get_transition_set(states[i],actions)\n",
    "        for next_state,prob,cost,arrival_time in transition_set:\n",
    "            P[actions,i,(states[:,0] == next_state[0]) & (states[:,1] == next_state[1])] = prob\n",
    "            R[actions,i,(states[:,0] == next_state[0]) & (states[:,1] == next_state[1])] = -cost\n",
    "\n",
    "time_for_discounting = {\n",
    "    0 : 1/asd.arrival_rate,\n",
    "    1 : asd.Tp,\n",
    "    2 : asd.Tnp\n",
    "}\n",
    "\n",
    "vi = ValueIterationUpdated(P1, R1,0.8)\n",
    "vi.run()\n",
    "result = vi.policy # result is (0, 0, 0)import mdptoolbox.example\n",
    "for i in range(len(states)):\n",
    "    print(f\"States:{states[i]}, Values:{vi.V[i]}, Policy:{result[i]}\")\n",
    "\n",
    "check = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e352dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom implementation is being used\n",
      "States:[0 0], Values:-0.10495008534279242, Policy:0\n",
      "States:[0 1], Values:-5.177615645832089, Policy:0\n",
      "States:[0 2], Values:-9.697491949956946, Policy:2\n",
      "States:[0 3], Values:-13.06015965472841, Policy:2\n",
      "States:[0 4], Values:-16.362753827236947, Policy:2\n",
      "States:[0 5], Values:-19.662228908676756, Policy:2\n",
      "States:[1 0], Values:-6.282549465259967, Policy:1\n",
      "States:[1 1], Values:-9.697491949956946, Policy:1\n",
      "States:[1 2], Values:-13.06015965472841, Policy:1\n",
      "States:[1 3], Values:-16.362753827236947, Policy:1\n",
      "States:[1 4], Values:-19.662228908676756, Policy:1\n",
      "States:[2 0], Values:-113.8847415107394, Policy:1\n",
      "States:[2 1], Values:-117.19124845342161, Policy:1\n",
      "States:[2 2], Values:-120.49384262593016, Policy:1\n",
      "States:[2 3], Values:-123.79331770736997, Policy:1\n",
      "States:[3 0], Values:-228.79350433679812, Policy:1\n",
      "States:[3 1], Values:-232.0932440357185, Policy:1\n",
      "States:[3 2], Values:-235.39271911715826, Policy:1\n",
      "States:[4 0], Values:-344.2284187363353, Policy:1\n",
      "States:[4 1], Values:-347.5277487841621, Policy:1\n",
      "States:[5 0], Values:-459.7011949826234, Policy:1\n"
     ]
    }
   ],
   "source": [
    "Q = 5\n",
    "asd = ValueIteration(Q,3,3,0.5,0.3,0.5,35, 1,0.2)\n",
    "transition_reward_matrix = asd.get_transition_reward_matrix()\n",
    "\n",
    "states = np.array([[i, j] for i in range(Q + 1) for j in range(Q - i + 1)])\n",
    "P2 = np.zeros((3,len(states),len(states)))\n",
    "R2 = np.zeros((3,len(states),len(states)))\n",
    "for actions in [0,1,2]:\n",
    "    for i in range(len(states)):\n",
    "        transition_set = transition_reward_matrix[tuple(states[i])][actions]\n",
    "        for next_state,prob,cost in transition_set:\n",
    "            P2[actions,i,(states[:,0] == next_state[0]) & (states[:,1] == next_state[1])] = prob\n",
    "            R2[actions,i,(states[:,0] == next_state[0]) & (states[:,1] == next_state[1])] = -cost\n",
    "\n",
    "\n",
    "time_for_discounting = {\n",
    "    0 : 1/asd.arrival_rate,\n",
    "    1 : asd.Tp,\n",
    "    2 : asd.Tnp\n",
    "}\n",
    "\n",
    "vi = ValueIterationUpdated(P2, R2,0.8)\n",
    "vi.run()\n",
    "result = vi.policy # result is (0, 0, 0)import mdptoolbox.example\n",
    "for i in range(len(states)):\n",
    "    print(f\"States:{states[i]}, Values:{vi.V[i]}, Policy:{result[i]}\")\n",
    "\n",
    "check = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
